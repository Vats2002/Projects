{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Create dimension tables\n",
    "CREATE TABLE products (\n",
    "    product_id INT PRIMARY KEY,\n",
    "    product_name VARCHAR(255),\n",
    "    category VARCHAR(255),\n",
    "    brand VARCHAR(255)\n",
    "    -- Add other product attributes\n",
    ");\n",
    "\n",
    "CREATE TABLE customers (\n",
    "    customer_id INT PRIMARY KEY,\n",
    "    customer_name VARCHAR(255),\n",
    "    address VARCHAR(255)\n",
    "    -- Add other customer attributes\n",
    ");\n",
    "\n",
    "CREATE TABLE time (\n",
    "    date DATE PRIMARY KEY,\n",
    "    year INT,\n",
    "    month INT,\n",
    "    day INT,\n",
    "    quarter INT\n",
    "    -- Add other time attributes\n",
    ");\n",
    "\n",
    "-- Create fact table\n",
    "CREATE TABLE sales (\n",
    "    product_id INT,\n",
    "    customer_id INT,\n",
    "    date DATE,\n",
    "    sales_amount DECIMAL(10, 2),\n",
    "    -- Add other measures and foreign keys to additional dimensions\n",
    "    FOREIGN KEY (product_id) REFERENCES products(product_id),\n",
    "    FOREIGN KEY (customer_id) REFERENCES customers(customer_id),\n",
    "    FOREIGN KEY (date) REFERENCES time(date)\n",
    ");\n",
    "\n",
    "-- Populate the fact table with sample data\n",
    "INSERT INTO sales (product_id, customer_id, date, sales_amount)\n",
    "VALUES\n",
    "    (1, 1, '2022-01-01', 100.00),\n",
    "    (2, 2, '2022-01-01', 50.00),\n",
    "    -- Add more sample data\n",
    "    ;\n",
    "\n",
    "\n",
    "SELECT p.product_name, SUM(s.sales_amount) AS total_sales\n",
    "FROM sales s\n",
    "JOIN products p ON s.product_id = p.product_id\n",
    "GROUP BY p.product_name;\n",
    "\n",
    "\n",
    "SELECT t.year, t.quarter, SUM(s.sales_amount) AS total_sales\n",
    "FROM sales s\n",
    "JOIN time t ON s.date = t.date\n",
    "GROUP BY t.year, t.quarter;\n",
    "\n",
    "\n",
    "SELECT t.date, s.sales_amount\n",
    "FROM sales s\n",
    "JOIN time t ON s.date = t.date\n",
    "WHERE s.product_id = 1\n",
    "    AND t.date BETWEEN '2022-01-01' AND '2022-12-31';\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import psycopg2\n",
    "\n",
    "# Extraction: Read data from CSV files\n",
    "def extract_data(csv_file):\n",
    "    extracted_data = []\n",
    "    with open(csv_file, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            extracted_data.append(row)\n",
    "    return extracted_data\n",
    "\n",
    "# Transformation: Apply business rules or calculations\n",
    "def transform_data(data):\n",
    "    transformed_data = []\n",
    "    for row in data:\n",
    "        # Apply transformations, e.g., calculate revenue based on quantity and price\n",
    "        revenue = float(row['quantity']) * float(row['price'])\n",
    "        row['revenue'] = revenue\n",
    "        transformed_data.append(row)\n",
    "    return transformed_data\n",
    "\n",
    "# Loading: Load transformed data into a data warehouse\n",
    "def load_data(data):\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        database=\"mydatawarehouse\",\n",
    "        user=\"myuser\",\n",
    "        password=\"mypassword\"\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    for row in data:\n",
    "        # Load data into the data warehouse table\n",
    "        query = f\"INSERT INTO mytable (product_id, quantity, price, revenue) VALUES ({row['product_id']}, {row['quantity']}, {row['price']}, {row['revenue']})\"\n",
    "        cursor.execute(query)\n",
    "    \n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "# Example usage\n",
    "csv_file = 'data.csv'  # Replace with your CSV file path\n",
    "\n",
    "# Extract data from CSV\n",
    "extracted_data = extract_data(csv_file)\n",
    "\n",
    "# Transform data\n",
    "transformed_data = transform_data(extracted_data)\n",
    "\n",
    "# Load transformed data into the data warehouse\n",
    "load_data(transformed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Create dimension tables\n",
    "CREATE TABLE students (\n",
    "    student_id SERIAL PRIMARY KEY,\n",
    "    student_name VARCHAR(255),\n",
    "    student_major VARCHAR(255)\n",
    "    -- Add other student attributes\n",
    ");\n",
    "\n",
    "CREATE TABLE courses (\n",
    "    course_id SERIAL PRIMARY KEY,\n",
    "    course_name VARCHAR(255),\n",
    "    course_department VARCHAR(255)\n",
    "    -- Add other course attributes\n",
    ");\n",
    "\n",
    "CREATE TABLE time (\n",
    "    time_id SERIAL PRIMARY KEY,\n",
    "    date DATE,\n",
    "    semester VARCHAR(255),\n",
    "    year INT\n",
    "    -- Add other time attributes\n",
    ");\n",
    "\n",
    "-- Create fact table\n",
    "CREATE TABLE enrollments (\n",
    "    enrollment_id SERIAL PRIMARY KEY,\n",
    "    student_id INT,\n",
    "    course_id INT,\n",
    "    time_id INT,\n",
    "    -- Add other measures and foreign keys to additional dimensions\n",
    "    FOREIGN KEY (student_id) REFERENCES students(student_id),\n",
    "    FOREIGN KEY (course_id) REFERENCES courses(course_id),\n",
    "    FOREIGN KEY (time_id) REFERENCES time(time_id)\n",
    ");\n",
    "\n",
    "-- Insert sample data into dimension tables\n",
    "INSERT INTO students (student_name, student_major)\n",
    "VALUES\n",
    "    ('John Doe', 'Computer Science'),\n",
    "    ('Jane Smith', 'Psychology'),\n",
    "    -- Add more sample data\n",
    "    ;\n",
    "\n",
    "INSERT INTO courses (course_name, course_department)\n",
    "VALUES\n",
    "    ('Database Management', 'Computer Science'),\n",
    "    ('Intro to Psychology', 'Psychology'),\n",
    "    -- Add more sample data\n",
    "    ;\n",
    "\n",
    "INSERT INTO time (date, semester, year)\n",
    "VALUES\n",
    "    ('2022-01-01', 'Spring', 2022),\n",
    "    ('2022-06-01', 'Summer', 2022),\n",
    "    -- Add more sample data\n",
    "    ;\n",
    "\n",
    "-- Insert sample data into the fact table (enrollments)\n",
    "INSERT INTO enrollments (student_id, course_id, time_id)\n",
    "VALUES\n",
    "    (1, 1, 1),\n",
    "    (2, 2, 2),\n",
    "    -- Add more sample data\n",
    "    ;\n",
    "\n",
    "\n",
    "\n",
    "SELECT t.semester, COUNT(*) AS total_enrollments\n",
    "FROM enrollments e\n",
    "JOIN time t ON e.time_id = t.time_id\n",
    "GROUP BY t.semester;\n",
    "\n",
    "\n",
    "SELECT s.student_name, c.course_name\n",
    "FROM enrollments e\n",
    "JOIN students s ON e.student_id = s.student_id\n",
    "JOIN courses c ON e.course_id = c.course_id;\n",
    "\n",
    "\n",
    "SELECT c.course_department, COUNT(*) AS total_enrollments\n",
    "FROM enrollments e\n",
    "JOIN courses c ON e.course_id = c.course_id\n",
    "GROUP BY c.course_department;\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import csv\n",
    "from multiprocessing import Pool\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Function to load data in bulk using batch processing\n",
    "def load_data_batch(data):\n",
    "    # Connect to the database\n",
    "    client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "    database = client[\"mydatawarehouse\"]  # Replace with your database name\n",
    "    collection = database[\"mycollection\"]  # Replace with your collection name\n",
    "\n",
    "    # Insert data in bulk\n",
    "    collection.insert_many(data)\n",
    "\n",
    "# Function to process a batch of data\n",
    "def process_batch(batch):\n",
    "    # Transform data if required\n",
    "    processed_data = batch  # Placeholder, modify as per your transformation logic\n",
    "    load_data_batch(processed_data)\n",
    "\n",
    "# Function to load data using multi-threading or multiprocessing\n",
    "def load_data_parallel(data, batch_size, num_processes):\n",
    "    # Split data into batches\n",
    "    batches = [data[i:i+batch_size] for i in range(0, len(data), batch_size)]\n",
    "\n",
    "    # Create a pool of processes\n",
    "    pool = Pool(processes=num_processes)\n",
    "\n",
    "    # Start the parallel data loading process\n",
    "    pool.map(process_batch, batches)\n",
    "\n",
    "    # Close the pool\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "# Function to measure the time taken to load data\n",
    "def measure_loading_time(data, batch_size, num_processes):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Call the optimized data loading function\n",
    "    load_data_parallel(data, batch_size, num_processes)\n",
    "\n",
    "    end_time = time.time()\n",
    "    loading_time = end_time - start_time\n",
    "    print(f\"Data loading time: {loading_time} seconds\")\n",
    "\n",
    "# Example usage\n",
    "csv_file = 'data.csv'  # Replace with your CSV file path\n",
    "batch_size = 1000  # Number of records per batch\n",
    "num_processes = 4  # Number of parallel processes\n",
    "\n",
    "# Extract data from CSV\n",
    "data = []\n",
    "with open(csv_file, 'r') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        data.append(row)\n",
    "\n",
    "# Measure loading time before optimizations\n",
    "measure_loading_time(data, 1, 1)\n",
    "\n",
    "# Measure loading time after optimizations\n",
    "measure_loading_time(data, batch_size, num_processes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
